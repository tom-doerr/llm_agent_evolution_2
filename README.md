# LLM Agent Evolution

A framework for evolving LLM-based agents through evolutionary algorithms for any measurable goal.

## Overview

This project implements an evolutionary algorithm for LLM-based agents. The system evolves agents with three chromosomes:
- **Task chromosome**: The output that gets evaluated for fitness
- **Mate selection chromosome**: Instructions for selecting mates
- **Mutation chromosome**: Instructions for how to mutate chromosomes

The system uses a continuous evolution process rather than discrete generations, with parent selection using a Pareto distribution weighted by fitness squared.

## Key Features

- **Chromosomes for Separation of Functionality**: Each agent has three chromosomes to separate different aspects of functionality
- **Evolutionary Approach**: Everything evolves, including combination and mate selection strategies
- **Universal Adaptability**: Optimize for any goal expressible as a numerical reward
- **Command-Based Evaluation**: External commands receive agent output via stdin and return numerical rewards
- **Information-Dense Output**: Provides concise but informative output
- **Detailed Logging**: Logs detailed information to a file
- **Parent Selection**: Uses Pareto distribution weighting by fitness^2 with weighted sampling
- **Population Statistics**: Includes mean, median, standard deviation for population rewards
- **Large Population Support**: Default population size limit of one million
- **LLM-Based Mutation**: Mutations are generated by LLMs with specific instructions
- **Mate Selection**: Uses LLM to select mates based on agent's mate selection chromosome
- **Chromosome Hotspots**: Mating combines chromosomes at hotspots (punctuation, spaces)
- **Hexagonal Architecture**: Clean separation of concerns with domain logic isolated from external systems
- **Multithreading Support**: Run multiple evolution threads in parallel
- **Mock LLM Support**: Test without making real API calls
- **Standalone Mode**: Run without LLM API calls for simple optimization tasks

## Installation

### Prerequisites

- Python 3.8+
- pip

### Basic Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/llm_agent_evolution.git
cd llm_agent_evolution

# Install the package in development mode
pip install -e .

# Install with test dependencies
pip install -e ".[test]"
```

## Usage

### Quick Start

Run a quick test using the mock LLM adapter (no real API calls):

```bash
# Using the installed CLI tool
llm-evolve --quick-test

# Or using the module directly
python -m llm_agent_evolution --quick-test
```

### Main Evolution Process

Run the evolution process with a real LLM:

```bash
llm-evolve --population-size 50 --parallel-agents 8 --model "openrouter/google/gemini-2.0-flash-001"
```

### Universal Optimizer

Run the universal optimizer with a custom evaluation command:

```bash
llm-evolve optimize "python examples/count_a.py" --population-size 50 --parallel-agents 8
```

The universal optimizer allows you to optimize any text-based output using evolutionary algorithms. It works by:

1. Generating a population of text outputs
2. Evaluating each output using your custom command or script
3. Evolving the population through selection, mating, and mutation
4. Repeating until optimal solutions are found

The key innovation is the command-based evaluation interface, which allows you to define any optimization goal by providing a command that returns a numerical reward.

#### Evaluation Scripts

The evaluation script is the heart of the Universal Optimizer. It should:

1. Read input from stdin
2. Process the input in any way you want
3. Output a numerical reward as the last line of stdout

The reward should be higher for better solutions. The optimizer will try to maximize this value.

#### Example Evaluation Scripts

**Count 'a's in Text**

```python
#!/usr/bin/env python3
import sys

text = sys.stdin.read()
reward = text.count('a')
print(reward)
```

**Test Code Against Test Cases**

```python
#!/usr/bin/env python3
import sys
import ast

# Read code from stdin
code = sys.stdin.read()

# Define test cases
test_cases = [
    (5, 120),    # factorial(5) should be 120
    (0, 1),      # factorial(0) should be 1
    (1, 1),      # factorial(1) should be 1
    (10, 3628800) # factorial(10) should be 3628800
]

# Try to execute the code
try:
    # Check syntax
    ast.parse(code)
    
    # Create namespace and execute code
    namespace = {}
    exec(code, namespace)
    
    # Check if factorial function exists
    if 'factorial' not in namespace:
        print("Function 'factorial' not found")
        print(0)  # Reward
        sys.exit(0)
    
    # Run test cases
    passing = 0
    for input_val, expected in test_cases:
        try:
            result = namespace['factorial'](input_val)
            if result == expected:
                passing += 1
        except Exception:
            pass
    
    # Calculate reward
    reward = passing / len(test_cases) * 10
    
    # Print results and reward
    print(f"Passing tests: {passing}/{len(test_cases)}")
    print(reward)
    
except Exception as e:
    print(f"Error: {e}")
    print(0)  # Reward
```

#### Advanced Usage

**Using Initial Content**

You can provide initial content to seed the optimization:

```bash
llm-evolve optimize --eval-command "python examples/count_a.py" --initial-content "Starting text"
```

Or from a file:

```bash
llm-evolve optimize --eval-command "python examples/count_a.py" --initial-file my_starting_point.txt
```

**Saving Results**

Save the best result to a file:

```bash
llm-evolve optimize --eval-command "python examples/count_a.py" --output-file best_result.txt
```

### Standalone Optimizer (No LLM API Calls)

Run the simplified standalone optimizer that doesn't use LLM API calls:

```bash
llm-evolve standalone "python examples/count_a.py" --population-size 50 --parallel-agents 8
```

### Interactive Demo

Run an interactive step-by-step demo of the evolution process:

```bash
llm-evolve demo --use-mock
```

## How the Optimization Works

The system optimizes agents through an evolutionary process:

1. **Initialization**: Create a population of agents with empty chromosomes
2. **Evaluation**: Evaluate each agent's task chromosome to get a reward
3. **Parent Selection**: Select parents using Pareto distribution weighted by fitness^2
4. **Mating**: Combine chromosomes from parents at hotspots (punctuation, spaces)
5. **Mutation**: Use the agent's mutation chromosome as instructions for the LLM to modify it
6. **Population Management**: Add new agents to population, removing worst if size limit reached

## Command Line Options

### Main Evolution Process

```
llm-evolve [evolve] [options]

Options:
  -p, --population-size INT    Initial population size (default: 100)
  -j, --parallel-agents INT    Number of agents to evaluate in parallel (default: 10)
  -n, --max-evaluations INT    Maximum number of evaluations to run (default: unlimited)
  -m, --model STRING           LLM model to use (default: openrouter/google/gemini-2.0-flash-001)
  -l, --log-file STRING        Log file path (default: evolution.log)
  --use-mock, --mock           Use mock LLM adapter for testing
  -s, --seed INT               Random seed for reproducibility
  -e, --eval-command STRING    Command to run for evaluation
  -q, --quick-test             Run a quick test with mock LLM
```

### Universal Optimizer

```
llm-evolve optimize EVAL_COMMAND [options]

Options:
  -p, --population-size INT    Initial population size (default: 50)
  -j, --parallel-agents INT    Number of agents to evaluate in parallel (default: 8)
  -n, --max-evaluations INT    Maximum number of evaluations to run (default: unlimited)
  -m, --model STRING           LLM model to use (default: openrouter/google/gemini-2.0-flash-001)
  -l, --log-file STRING        Log file path (default: universal_optimize.log)
  --use-mock-llm, --mock       Use mock LLM adapter for testing
  -s, --seed INT               Random seed for reproducibility
  -t, --script-timeout INT     Maximum execution time for the evaluation script (default: 30)
  -i, --initial-content STRING Initial content for the chromosomes
  -f, --initial-file FILE      File containing initial content for the chromosomes
  -o, --output-file FILE       File to write the best result to
  --output-format FORMAT       Output format: text or json (default: text)
  --max-chars INT              Maximum number of characters for chromosomes (default: 1000)
  -v, --verbose                Enable verbose mode with detailed output
```

### Standalone Optimizer

```
llm-evolve standalone EVAL_COMMAND [options]

Options:
  -p, --population-size INT    Initial population size (default: 50)
  -j, --parallel-agents INT    Number of agents to evaluate in parallel (default: 8)
  -n, --max-evaluations INT    Maximum number of evaluations to run (default: 1000)
  -i, --initial-content STRING Initial content for the chromosomes
  -s, --seed INT               Random seed for reproducibility
  -v, --verbose                Enable verbose output
  -o, --output-file FILE       File to write the best result to
```

## Evaluation Scripts

The evaluation script is the heart of the Universal Optimizer. It should:

1. Read input from stdin
2. Process the input in any way you want
3. Output a numerical reward as the last line of stdout

The reward should be higher for better solutions. The optimizer will try to maximize this value.

### Example Evaluation Scripts

#### Count 'a's in Text

```python
#!/usr/bin/env python3
import sys

text = sys.stdin.read()
reward = text.count('a')
print(reward)
```

#### Test Code Against Test Cases

```python
#!/usr/bin/env python3
import sys
import ast

# Read code from stdin
code = sys.stdin.read()

# Define test cases
test_cases = [
    (5, 120),    # factorial(5) should be 120
    (0, 1),      # factorial(0) should be 1
    (1, 1),      # factorial(1) should be 1
    (10, 3628800) # factorial(10) should be 3628800
]

# Try to execute the code
try:
    # Check syntax
    ast.parse(code)
    
    # Create namespace and execute code
    namespace = {}
    exec(code, namespace)
    
    # Check if factorial function exists
    if 'factorial' not in namespace:
        print("Function 'factorial' not found")
        print(0)  # Reward
        sys.exit(0)
    
    # Run test cases
    passing = 0
    for input_val, expected in test_cases:
        try:
            result = namespace['factorial'](input_val)
            if result == expected:
                passing += 1
        except Exception:
            pass
    
    # Calculate reward
    reward = passing / len(test_cases) * 10
    
    # Print results and reward
    print(f"Passing tests: {passing}/{len(test_cases)}")
    print(reward)
    
except Exception as e:
    print(f"Error: {e}")
    print(0)  # Reward
```

#### Optimize Text Readability

```python
#!/usr/bin/env python3
import sys
import textstat

text = sys.stdin.read()

# Calculate readability metrics
flesch_reading_ease = textstat.flesch_reading_ease(text)
flesch_kincaid_grade = textstat.flesch_kincaid_grade(text)

# Normalize and combine metrics
# Higher Flesch Reading Ease is better (easier to read)
# Lower Flesch-Kincaid Grade is better (lower grade level required)
normalized_grade = max(0, 20 - flesch_kincaid_grade) / 20
normalized_ease = flesch_reading_ease / 100

# Calculate combined reward
reward = (normalized_grade + normalized_ease) * 5

print(f"Flesch Reading Ease: {flesch_reading_ease}")
print(f"Flesch-Kincaid Grade: {flesch_kincaid_grade}")
print(reward)
```

## Advanced Usage

### Using Initial Content

You can provide initial content to seed the optimization:

```bash
llm-evolve optimize --eval-script my_eval_script.py --initial-content "Starting text"
```

Or from a file:

```bash
llm-evolve optimize --eval-script my_eval_script.py --initial-file my_starting_point.txt
```

### Saving Results

Save the best result to a file:

```bash
llm-evolve optimize --eval-script my_eval_script.py --output-file best_result.txt
```

Save detailed results in JSON format:

```bash
llm-evolve optimize --eval-script my_eval_script.py --output-file results.json --output-format json
```

### Using with DSPy

You can use the Universal Optimizer to optimize DSPy prompts:

```python
#!/usr/bin/env python3
import sys
import dspy

# Read prompt from stdin
prompt = sys.stdin.read()

# Create DSPy program with the prompt
lm = dspy.LM('openrouter/google/gemini-2.0-flash-001')
program = dspy.Predict("Question -> Answer", prompt)

# Define test cases
test_cases = [
    {"Question": "What is the capital of France?", "Answer": "Paris"},
    {"Question": "Who wrote Romeo and Juliet?", "Answer": "William Shakespeare"},
    # Add more test cases...
]

# Evaluate the prompt
correct = 0
for test_case in test_cases:
    try:
        prediction = program(dspy.Example(Question=test_case["Question"]))
        if test_case["Answer"].lower() in prediction.Answer.lower():
            correct += 1
    except Exception:
        pass

# Calculate reward
reward = correct / len(test_cases) * 10

# Print results and reward
print(f"Correct answers: {correct}/{len(test_cases)}")
print(reward)
```

## Architecture

The project follows a hexagonal (ports and adapters) architecture:

- **Domain**: Core entities and business logic
  - `model.py`: Agent and Chromosome classes
  - `services.py`: Evolution services like parent selection and mating

- **Ports**: Interface definitions
  - `primary.py`: Use case interfaces (evolution)
  - `secondary.py`: External system interfaces (LLM, logging, statistics)

- **Adapters**: Implementation of interfaces
  - Primary: CLI interface
  - Secondary: LLM, logging, statistics implementations

## Development

```bash
# Run tests
python -m pytest

# Run a specific test
python -m pytest tests/test_domain.py

# Run with coverage
python -m pytest --cov=llm_agent_evolution
```

## License

MIT
